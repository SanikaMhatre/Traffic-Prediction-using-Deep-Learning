{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OfBUSx4nIVnW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "import sqlite3\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(file_path='traffic_volume_data.csv'):\n",
        "    \"\"\"\n",
        "    Preprocess the traffic volume data and store in SQLite database\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Sort by date and time\n",
        "    data = data.sort_values(by=['date_time'], ascending=True).reset_index(drop=True)\n",
        "\n",
        "    # Create lag features for the last n hours\n",
        "    last_n_hours = [1, 2, 3, 4, 5, 6]\n",
        "    for n in last_n_hours:\n",
        "        data[f'last_{n}_hour_traffic'] = data['traffic_volume'].shift(n)\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    data = data.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Convert holiday to binary\n",
        "    data.loc[data['is_holiday'] != 'None', 'is_holiday'] = 1\n",
        "    data.loc[data['is_holiday'] == 'None', 'is_holiday'] = 0\n",
        "    data['is_holiday'] = data['is_holiday'].astype(int)\n",
        "\n",
        "    # Extract datetime features\n",
        "    data['date_time'] = pd.to_datetime(data['date_time'])\n",
        "    data['hour'] = data['date_time'].map(lambda x: int(x.strftime(\"%H\")))\n",
        "    data['month_day'] = data['date_time'].map(lambda x: int(x.strftime(\"%d\")))\n",
        "    data['weekday'] = data['date_time'].map(lambda x: x.weekday()+1)\n",
        "    data['month'] = data['date_time'].map(lambda x: int(x.strftime(\"%m\")))\n",
        "    data['year'] = data['date_time'].map(lambda x: int(x.strftime(\"%Y\")))\n",
        "\n",
        "    # Convert weather types and descriptions to numerical values\n",
        "    weather_type_mapping = {\n",
        "        'Rain': 1, 'Clouds': 2, 'Clear': 3, 'Snow': 4, 'Mist': 5,\n",
        "        'Drizzle': 6, 'Haze': 7, 'Thunderstorm': 8, 'Fog': 9, 'Smoke': 10, 'Squall': 11\n",
        "    }\n",
        "\n",
        "    weather_desc_mapping = {\n",
        "        'SQUALLS': 1, 'Sky is Clear': 2, 'broken clouds': 3, 'drizzle': 4,\n",
        "        'few clouds': 5, 'fog': 6, 'freezing rain': 7, 'haze': 8,\n",
        "        'heavy intensity drizzle': 9, 'heavy intensity rain': 10, 'heavy snow': 11,\n",
        "        'light intensity drizzle': 12, 'light intensity shower rain': 13, 'light rain': 14,\n",
        "        'light rain and snow': 15, 'light shower snow': 16, 'light snow': 17,\n",
        "        'mist': 18, 'moderate rain': 19, 'overcast clouds': 20,\n",
        "        'proximity shower rain': 21, 'proximity thunderstorm': 22,\n",
        "        'proximity thunderstorm with drizzle': 23, 'proximity thunderstorm with rain': 24,\n",
        "        'scattered clouds': 25, 'shower snow': 26, 'sky is clear': 27,\n",
        "        'sleet': 28, 'smoke': 29, 'snow': 30, 'thunderstorm': 31,\n",
        "        'thunderstorm with drizzle': 32, 'thunderstorm with heavy rain': 33,\n",
        "        'thunderstorm with light drizzle': 34, 'thunderstorm with light rain': 35,\n",
        "        'thunderstorm with rain': 36, 'very heavy rain': 37\n",
        "    }\n",
        "\n",
        "    # Apply mappings with a default of 0 for any value not in the mapping\n",
        "    data['weather_type_num'] = data['weather_type'].map(\n",
        "        lambda x: weather_type_mapping.get(x, 0))\n",
        "    data['weather_description_num'] = data['weather_description'].map(\n",
        "        lambda x: weather_desc_mapping.get(x, 0))\n",
        "\n",
        "    # Save preprocessed data to CSV\n",
        "    data.to_csv(\"cleaned_data.csv\", index=None)\n",
        "\n",
        "    # Return processed data\n",
        "    return data"
      ],
      "metadata": {
        "id": "lBKjajj8IbcT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lstm_model(input_shape):\n",
        "    \"\"\"Create and compile LSTM model\"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "kwqVIlr2I6tp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lstm_data(data, lookback=6, feature_cols=None, target_col='traffic_volume'):\n",
        "    \"\"\"\n",
        "    Prepare data for LSTM model with lookback sequence\n",
        "    \"\"\"\n",
        "    if feature_cols is None:\n",
        "        feature_cols = ['is_holiday', 'humidity', 'wind_speed', 'temperature',\n",
        "                        'weekday', 'hour', 'month_day', 'year', 'month']\n",
        "\n",
        "    # Extract features and target\n",
        "    X = data[feature_cols].values\n",
        "    y = data[target_col].values\n",
        "\n",
        "    # Scale features\n",
        "    x_scaler = MinMaxScaler()\n",
        "    X = x_scaler.fit_transform(X)\n",
        "\n",
        "    # Scale target\n",
        "    y_scaler = MinMaxScaler()\n",
        "    y = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Save scalers for later use in prediction\n",
        "    with open('x_scaler.pkl', 'wb') as file:\n",
        "        pickle.dump(x_scaler, file)\n",
        "\n",
        "    with open('y_scaler.pkl', 'wb') as file:\n",
        "        pickle.dump(y_scaler, file)\n",
        "\n",
        "    # Create sequences for LSTM\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - lookback):\n",
        "        X_seq.append(X[i:i+lookback])\n",
        "        y_seq.append(y[i+lookback])\n",
        "\n",
        "    X_seq = np.array(X_seq)\n",
        "    y_seq = np.array(y_seq)\n",
        "\n",
        "    return X_seq, y_seq, x_scaler, y_scaler"
      ],
      "metadata": {
        "id": "NaUphgLAJEBC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(data, epochs=50, batch_size=32, validation_split=0.2):\n",
        "    \"\"\"Train LSTM model and save it\"\"\"\n",
        "    # Define feature columns\n",
        "    feature_cols = ['is_holiday', 'humidity', 'wind_speed', 'temperature',\n",
        "                    'weekday', 'hour', 'month_day', 'year', 'month']\n",
        "\n",
        "    # Prepare data for LSTM\n",
        "    X_seq, y_seq, x_scaler, y_scaler = prepare_lstm_data(data, lookback=6,\n",
        "                                                       feature_cols=feature_cols)\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq,\n",
        "                                                      test_size=validation_split,\n",
        "                                                      random_state=42)\n",
        "\n",
        "    # Create and train model\n",
        "    model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save('traffic_lstm_model.h5')\n",
        "    print(\"Model trained and saved successfully!\")\n",
        "\n",
        "    # Evaluate model\n",
        "    train_loss, train_mae = model.evaluate(X_train, y_train, verbose=0)\n",
        "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "    print(f\"Training MAE: {train_mae:.4f}\")\n",
        "    print(f\"Validation MAE: {val_mae:.4f}\")"
      ],
      "metadata": {
        "id": "3xiHf0RMJOTs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Process data\n",
        "    data = preprocess_data('/content/drive/MyDrive/traffic_prediction_6/traffic_volume_data.csv')\n",
        "\n",
        "    # Train and save model\n",
        "    train_model(data, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rdyK3WCJfPc",
        "outputId": "078f153b-e62d-41a7-c4eb-4d983d5bd3e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.0719 - mae: 0.2088 - val_loss: 0.0289 - val_mae: 0.1266\n",
            "Epoch 2/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0280 - mae: 0.1222 - val_loss: 0.0240 - val_mae: 0.1106\n",
            "Epoch 3/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 0.0249 - mae: 0.1145 - val_loss: 0.0219 - val_mae: 0.1044\n",
            "Epoch 4/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0220 - mae: 0.1078 - val_loss: 0.0178 - val_mae: 0.0917\n",
            "Epoch 5/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 0.0185 - mae: 0.0981 - val_loss: 0.0157 - val_mae: 0.0866\n",
            "Epoch 6/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.0157 - mae: 0.0891 - val_loss: 0.0154 - val_mae: 0.0824\n",
            "Epoch 7/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0137 - mae: 0.0813 - val_loss: 0.0127 - val_mae: 0.0776\n",
            "Epoch 8/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0130 - mae: 0.0785 - val_loss: 0.0116 - val_mae: 0.0750\n",
            "Epoch 9/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0116 - mae: 0.0736 - val_loss: 0.0124 - val_mae: 0.0772\n",
            "Epoch 10/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.0110 - mae: 0.0714 - val_loss: 0.0117 - val_mae: 0.0724\n",
            "Epoch 11/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0105 - mae: 0.0693 - val_loss: 0.0100 - val_mae: 0.0681\n",
            "Epoch 12/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0105 - mae: 0.0688 - val_loss: 0.0096 - val_mae: 0.0650\n",
            "Epoch 13/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0105 - mae: 0.0686 - val_loss: 0.0104 - val_mae: 0.0717\n",
            "Epoch 14/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0100 - mae: 0.0671 - val_loss: 0.0096 - val_mae: 0.0678\n",
            "Epoch 15/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0100 - mae: 0.0668 - val_loss: 0.0087 - val_mae: 0.0609\n",
            "Epoch 16/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: 0.0095 - mae: 0.0647 - val_loss: 0.0092 - val_mae: 0.0633\n",
            "Epoch 17/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0095 - mae: 0.0647 - val_loss: 0.0089 - val_mae: 0.0621\n",
            "Epoch 18/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0090 - mae: 0.0627 - val_loss: 0.0088 - val_mae: 0.0600\n",
            "Epoch 19/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0088 - mae: 0.0620 - val_loss: 0.0084 - val_mae: 0.0584\n",
            "Epoch 20/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0089 - mae: 0.0625 - val_loss: 0.0088 - val_mae: 0.0629\n",
            "Epoch 21/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0085 - mae: 0.0602 - val_loss: 0.0091 - val_mae: 0.0640\n",
            "Epoch 22/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0084 - mae: 0.0607 - val_loss: 0.0097 - val_mae: 0.0644\n",
            "Epoch 23/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0086 - mae: 0.0607 - val_loss: 0.0089 - val_mae: 0.0611\n",
            "Epoch 24/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 0.0085 - mae: 0.0605 - val_loss: 0.0086 - val_mae: 0.0618\n",
            "Epoch 25/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.0086 - mae: 0.0604 - val_loss: 0.0090 - val_mae: 0.0635\n",
            "Epoch 26/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0082 - mae: 0.0596 - val_loss: 0.0092 - val_mae: 0.0630\n",
            "Epoch 27/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0083 - mae: 0.0595 - val_loss: 0.0084 - val_mae: 0.0611\n",
            "Epoch 28/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0082 - mae: 0.0594 - val_loss: 0.0080 - val_mae: 0.0561\n",
            "Epoch 29/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: 0.0083 - mae: 0.0579 - val_loss: 0.0083 - val_mae: 0.0594\n",
            "Epoch 30/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.0080 - mae: 0.0583 - val_loss: 0.0088 - val_mae: 0.0629\n",
            "Epoch 31/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 0.0078 - mae: 0.0577 - val_loss: 0.0089 - val_mae: 0.0655\n",
            "Epoch 32/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.0079 - mae: 0.0575 - val_loss: 0.0082 - val_mae: 0.0581\n",
            "Epoch 33/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0081 - mae: 0.0577 - val_loss: 0.0079 - val_mae: 0.0556\n",
            "Epoch 34/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0078 - mae: 0.0569 - val_loss: 0.0081 - val_mae: 0.0586\n",
            "Epoch 35/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: 0.0080 - mae: 0.0575 - val_loss: 0.0094 - val_mae: 0.0664\n",
            "Epoch 36/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 0.0079 - mae: 0.0577 - val_loss: 0.0086 - val_mae: 0.0584\n",
            "Epoch 37/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0573 - val_loss: 0.0084 - val_mae: 0.0604\n",
            "Epoch 38/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0079 - mae: 0.0577 - val_loss: 0.0091 - val_mae: 0.0635\n",
            "Epoch 39/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0077 - mae: 0.0566 - val_loss: 0.0084 - val_mae: 0.0581\n",
            "Epoch 40/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.0080 - mae: 0.0576 - val_loss: 0.0081 - val_mae: 0.0558\n",
            "Epoch 41/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0078 - mae: 0.0560 - val_loss: 0.0091 - val_mae: 0.0649\n",
            "Epoch 42/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.0079 - mae: 0.0570 - val_loss: 0.0092 - val_mae: 0.0674\n",
            "Epoch 43/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 0.0077 - mae: 0.0569 - val_loss: 0.0080 - val_mae: 0.0579\n",
            "Epoch 44/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 0.0073 - mae: 0.0549 - val_loss: 0.0081 - val_mae: 0.0589\n",
            "Epoch 45/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 0.0075 - mae: 0.0557 - val_loss: 0.0081 - val_mae: 0.0567\n",
            "Epoch 46/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - loss: 0.0073 - mae: 0.0551 - val_loss: 0.0079 - val_mae: 0.0559\n",
            "Epoch 47/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - loss: 0.0075 - mae: 0.0555 - val_loss: 0.0080 - val_mae: 0.0579\n",
            "Epoch 48/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - loss: 0.0074 - mae: 0.0560 - val_loss: 0.0081 - val_mae: 0.0582\n",
            "Epoch 49/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - loss: 0.0070 - mae: 0.0539 - val_loss: 0.0082 - val_mae: 0.0597\n",
            "Epoch 50/50\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - loss: 0.0074 - mae: 0.0552 - val_loss: 0.0083 - val_mae: 0.0590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully!\n",
            "Training MAE: 0.0564\n",
            "Validation MAE: 0.0590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/traffic_lstm_model.h5', compile=False)\n"
      ],
      "metadata": {
        "id": "2MIbBrrobNLI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import json\n",
        "\n",
        "def save_model_components():\n",
        "    \"\"\"\n",
        "    Save model architecture and weights separately\n",
        "    to handle loading issues with custom metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load model without compiling to avoid metric deserialization issues\n",
        "        model = load_model('/content/traffic_lstm_model.h5', compile=False)\n",
        "\n",
        "        # Save model architecture to JSON\n",
        "        model_json = model.to_json()\n",
        "        with open('/content/drive/MyDrive/traffic_prediction_6/model_architecture.json', 'w') as json_file:\n",
        "            json_file.write(model_json)\n",
        "\n",
        "        # Save weights separately\n",
        "        model.save_weights('/content/drive/MyDrive/traffic_prediction_6/model_weights.weights.h5')\n",
        "\n",
        "        print(\"Model architecture and weights saved successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model components: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_model_components()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f071dc3ejYs",
        "outputId": "794e921b-eebc-4d6d-b49c-908ccc43e13e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architecture and weights saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVwJJuCjeneY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}